{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_-t6ynlbRKCz"},"outputs":[],"source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!git clone https://github.com/WongKinYiu/yolov7.git"],"metadata":{"id":"0pnj_zM32AD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xghh6PcQQBje"},"outputs":[],"source":["!pip install -r yolov7/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMFqp88YTpDP"},"outputs":[],"source":["!pip install torchvision transformers"]},{"cell_type":"code","source":["import os\n","import scipy.io\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","\n","class KeypointVHS_Dataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.image_files = sorted(os.listdir(image_dir))\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        image_path = os.path.join(self.image_dir, self.image_files[idx])\n","        image = Image.open(image_path).convert(\"RGB\")\n","\n","        # Load label\n","        label_path = os.path.join(self.label_dir, os.path.splitext(self.image_files[idx])[0] + '.mat')\n","        mat = scipy.io.loadmat(label_path)\n","        keypoints = mat['six_points'].flatten().astype('float32')  # Flatten 6 (x, y) pairs into 12 values\n","\n","        # Calculate VHS class based on the VHS value\n","        vhs_value = mat['VHS'][0][0]  # Access VHS value\n","        if vhs_value < 8.2:\n","            vhs_class = 0\n","        elif 8.2 <= vhs_value <= 10:\n","            vhs_class = 1\n","        else:\n","            vhs_class = 2\n","\n","        # Apply transformations to the image, if specified\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Prepare label dictionary\n","        labels = {\n","            'keypoints_output': torch.tensor(keypoints, dtype=torch.float32),\n","            'vhs_class_output': torch.tensor(vhs_class, dtype=torch.long)\n","        }\n","\n","        return image, labels\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Define paths\n","train_image_dir = '/content/drive/MyDrive/Train/Images'\n","train_label_dir = '/content/drive/MyDrive/Train/Labels'\n","valid_image_dir = '/content/drive/MyDrive/Valid/Images'\n","valid_label_dir = '/content/drive/MyDrive/Valid/Labels'\n","\n","# Create datasets\n","train_dataset = KeypointVHS_Dataset(train_image_dir, train_label_dir, transform=transform)\n","valid_dataset = KeypointVHS_Dataset(valid_image_dir, valid_label_dir, transform=transform)\n","\n","# Create DataLoaders\n","batch_size = 3\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"gw7pbEqSEdBr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7dXQyXw_TebD"},"source":["using unet with attention gate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCvVoq7ZzMmc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# U-Net Encoder with Attention Gates\n","class AttentionGate(nn.Module):\n","    def __init__(self, F_g, F_l, F_int):\n","        super(AttentionGate, self).__init__()\n","        self.W_g = nn.Sequential(\n","            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.W_x = nn.Sequential(\n","            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(F_int)\n","        )\n","\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, g, x):\n","        # Resize g to match x's dimensions if they don't match\n","        if g.size() != x.size():\n","            g = F.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=True)\n","\n","        g1 = self.W_g(g)\n","        x1 = self.W_x(x)\n","        psi = self.relu(g1 + x1)\n","        psi = self.psi(psi)\n","        return x * psi\n","\n","class UNetEncoderWithAttention(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(UNetEncoderWithAttention, self).__init__()\n","        self.enc1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.enc2 = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.enc3 = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.enc4 = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.attention4 = AttentionGate(512, 256, 256)\n","\n","    def forward(self, x):\n","        flow = self.enc1(x)\n","        #print(\"Shape after enc1 (flow):\", flow.shape)\n","        x2 = self.enc2(flow)\n","        #print(\"Shape after enc2:\", x2.shape)\n","        x3 = self.enc3(x2)\n","        #print(\"Shape after enc3:\", x3.shape)\n","        fhigh = self.enc4(x3)\n","        #print(\"Shape after enc4 (before attention):\", fhigh.shape)\n","\n","        # Apply attention to high-level features\n","        fhigh = self.attention4(fhigh, x3)\n","        #print(\"Shape after attention (fhigh):\", fhigh.shape)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(64, 16, kernel_size=1)\n","        self.conv2 = nn.Conv2d(256, 16, kernel_size=1)\n","\n","    def forward(self, flow, fhigh):\n","        #print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        #print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","        #print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        #print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat (56x56)\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        #print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithUNetAttention(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithUNetAttention, self).__init__()\n","        self.encoder = UNetEncoderWithAttention()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithUNetAttention().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"markdown","metadata":{"id":"Refit4e9TWhj"},"source":["mobilenet and efficientnet for feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_a71GD5qK7mt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import mobilenet_v2, efficientnet_b0\n","\n","# Combined MobileNet + EfficientNet Encoder\n","class MobileNetEfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(MobileNetEfficientNetEncoder, self).__init__()\n","\n","        # Load MobileNet for low-level features\n","        mobilenet = mobilenet_v2(pretrained=True)\n","        self.low_level_features = nn.Sequential(*list(mobilenet.features[:7]))  # Use early layers for low-level features\n","\n","        # Add a 1x1 conv layer to adjust channels from 32 to 80\n","        self.channel_adjust = nn.Conv2d(in_channels=32, out_channels=80, kernel_size=1)\n","\n","        # Load EfficientNet for high-level features\n","        efficientnet = efficientnet_b0(pretrained=True)\n","        self.high_level_features = nn.Sequential(*list(efficientnet.features[5:]))  # Use deeper layers for high-level features\n","\n","        # Attention Gate\n","        self.attention = nn.Sequential(\n","            nn.Conv2d(1280, 320, kernel_size=1),\n","            nn.BatchNorm2d(320),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(320, 1, kernel_size=1),\n","            nn.BatchNorm2d(1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        flow = self.low_level_features(x)\n","        #print(\"Shape of flow (low-level features):\", flow.shape)\n","\n","        # Adjust channels to 80 before passing to high-level features\n","        flow = self.channel_adjust(flow)\n","\n","        # Extract high-level features\n","        fhigh = self.high_level_features(flow)\n","        #print(\"Shape of fhigh (high-level features before attention):\", fhigh.shape)\n","\n","        # Attention mechanism on high-level features\n","        attention_weights = self.attention(fhigh)\n","        fhigh = fhigh * attention_weights\n","        #print(\"Shape of fhigh (after attention):\", fhigh.shape)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(80, 16, kernel_size=1)  # Adjust channels to 80\n","        self.conv2 = nn.Conv2d(1280, 16, kernel_size=1)  # Adjust channels based on EfficientNet output\n","\n","    def forward(self, flow, fhigh):\n","        #print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        #print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        #print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        #print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        #print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithMobileEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithMobileEfficientNet, self).__init__()\n","        self.encoder = MobileNetEfficientNetEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithMobileEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"markdown","metadata":{"id":"lsfq3IPkTP4L"},"source":["using efficientnet_b7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qE9Lnyw6Jncp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import efficientnet_b7  # Use EfficientNet-B3 for deeper feature extraction\n","\n","# EfficientNet Encoder (using EfficientNet-B3 for added complexity)\n","class EfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetEncoder, self).__init__()\n","\n","        # Load EfficientNet-B3 for feature extraction\n","        efficientnet = efficientnet_b7(pretrained=True)\n","\n","        # Use early layers for low-level features\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:8]))  # Adjust as necessary for low-level features\n","\n","        # Use deeper layers for high-level features\n","        self.high_level_features = nn.Sequential(*list(efficientnet.features[:2]))  # Adjust as necessary for high-level features\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        #fhigh = self.high_level_features(x)\n","        #flow = self.low_level_features(fhigh)\n","        flow = self.low_level_features(x)\n","        #print(\"Shape of flow (low-level features):\", flow.shape)\n","\n","        # Extract high-level features\n","        fhigh = self.high_level_features(x)\n","        #print(\"Shape of fhigh (high-level features):\", fhigh.shape)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(640, 16, kernel_size=1)  # Adjust channels based on EfficientNet low-level output\n","        self.conv2 = nn.Conv2d(32, 16, kernel_size=1)  # Adjust channels based on EfficientNet high-level output\n","\n","    def forward(self, flow, fhigh):\n","        #print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        #print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        #print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        #print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        #print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithEfficientNet, self).__init__()\n","        self.encoder = EfficientNetEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")\n"]},{"cell_type":"code","source":["# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")"],"metadata":{"id":"k_cu-E6Qptt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ks4l2f99W-gU"},"source":["using FPN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIXJDa7xW8_b"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import efficientnet_b7  # Use EfficientNet-B3 for deeper feature extraction\n","from torchvision.ops import FeaturePyramidNetwork\n","\n","class EfficientNetWithFPN(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetWithFPN, self).__init__()\n","\n","        # Load EfficientNet-B3 as the backbone\n","        efficientnet = efficientnet_b7(pretrained=True)\n","\n","        # Define layers for different scales\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:3]))  # Fine details\n","        self.mid_level_features = nn.Sequential(*list(efficientnet.features[3:6])) # Mid-level features\n","        self.high_level_features = nn.Sequential(*list(efficientnet.features[6:])) # Broad context\n","\n","        # Create FPN on top of extracted features\n","        self.fpn = FeaturePyramidNetwork(in_channels_list=[40, 112, 1280], out_channels=256)\n","\n","    def forward(self, x):\n","        # Extract features at multiple levels\n","        low = self.low_level_features(x)\n","        mid = self.mid_level_features(low)\n","        high = self.high_level_features(mid)\n","\n","        # Pass features through the FPN\n","        fpn_out = self.fpn({\"0\": low, \"1\": mid, \"2\": high})\n","\n","        # Extract low-level and high-level feature maps\n","        low_level_output = fpn_out[\"0\"]  # Using the lowest level output from FPN\n","        high_level_output = fpn_out[\"2\"] # Using the highest level output from FPN\n","\n","        return low_level_output, high_level_output\n","\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(64, 16, kernel_size=1)  # Adjust channels based on EfficientNet low-level output\n","        self.conv2 = nn.Conv2d(64, 16, kernel_size=1)  # Adjust channels based on EfficientNet high-level output\n","\n","    def forward(self, flow, fhigh):\n","        print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithEfficientNet, self).__init__()\n","        self.encoder = EfficientNetWithFPN()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(256 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8VK6ZU9Tf8_Z"},"outputs":[],"source":["# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8-EAgzxOVDo"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import efficientnet_b7  # Use EfficientNet-B3 for deeper feature extraction\n","\n","# EfficientNet Encoder (using EfficientNet-B3 for added complexity)\n","class EfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetEncoder, self).__init__()\n","\n","        # Load EfficientNet-B3 for feature extraction\n","        efficientnet = efficientnet_b7(pretrained=True)\n","\n","        # Use early layers for low-level features\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:2]))  # Adjust as necessary for low-level features\n","\n","        # Use deeper layers for high-level features\n","        self.high_level_features = nn.Sequential(*list(efficientnet.features[2:4]))  # Adjust as necessary for high-level features\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        flow = self.low_level_features(x)\n","        print(\"Shape of flow (low-level features):\", flow.shape)\n","\n","        # Extract high-level features\n","        fhigh = self.high_level_features(flow)\n","        print(\"Shape of fhigh (high-level features):\", fhigh.shape)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(32, 16, kernel_size=1)  # Adjust channels based on EfficientNet low-level output\n","        self.conv2 = nn.Conv2d(80, 16, kernel_size=1)  # Adjust channels based on EfficientNet high-level output\n","\n","    def forward(self, flow, fhigh):\n","        print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithEfficientNet, self).__init__()\n","        self.encoder = EfficientNetEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","\n","\n","# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"q-HHNhX3FRdu"},"source":["combination of rvt and efficientnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7QgymaQEWLU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import efficientnet_b3  # Use EfficientNet-B3 for deeper feature extraction\n","\n","# EfficientNet Encoder (using EfficientNet-B3 for added complexity)\n","class EfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetEncoder, self).__init__()\n","        # Load EfficientNet-B3 for feature extraction\n","        efficientnet = efficientnet_b3(pretrained=True)\n","        # Use early layers for low-level features\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:1]))  # Adjust as necessary for low-level features\n","\n","        # Use deeper layers for high-level features\n","        self.layer1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.layer3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n","        self.layer4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        flow = self.low_level_features(x)\n","        print(\"Shape of flow (low-level features):\", flow.shape)\n","\n","        x = self.layer1(x)\n","        fhigh = self.layer2(x)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(32, 16, kernel_size=1)  # Adjust channels based on EfficientNet low-level output\n","        self.conv2 = nn.Conv2d(24, 16, kernel_size=1)  # Adjust channels based on EfficientNet high-level output\n","\n","    def forward(self, flow, fhigh):\n","        print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithEfficientNet, self).__init__()\n","        self.encoder = EfficientNetEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nloWBMJF7IfZ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision.models import efficientnet_b7\n","\n","class EfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetEncoder, self).__init__()\n","\n","        # Load EfficientNet-B3 for feature extraction\n","        efficientnet = efficientnet_b7(pretrained=True)\n","\n","        # Use early layers for low-level features\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:1]))  # Adjust as needed\n","\n","        # Use deeper layers for high-level features\n","        self.high_level_features = nn.Sequential(*list(efficientnet.features[2:4]))  # Adjust as needed\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        low_level_features = self.low_level_features(x)\n","\n","        # Extract high-level features\n","        high_level_features = self.high_level_features(low_level_features)\n","\n","        return low_level_features, high_level_features\n","\n","# Function to visualize feature maps\n","def visualize_feature_maps(features, title=\"Feature Maps\"):\n","    num_features = min(8, features.shape[1])  # Choose up to 8 feature maps for display\n","    fig, axs = plt.subplots(1, num_features, figsize=(15, 15))\n","    for i in range(num_features):\n","        axs[i].imshow(features[0, i].cpu().detach().numpy(), cmap='viridis')\n","        axs[i].axis('off')\n","    plt.suptitle(title)\n","    plt.show()\n","\n","# Load and preprocess a sample MRI image\n","def load_image(img_path):\n","    image = Image.open(img_path).convert('RGB')\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    image = transform(image).unsqueeze(0)  # Add batch dimension\n","    return image\n","\n","# Load sample image\n","image_path = '/content/drive/MyDrive/Train/Images/159.png'  # Update this path\n","image = load_image(image_path)\n","\n","# Instantiate model and move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = EfficientNetEncoder().to(device)\n","\n","# Pass the sample image through the model\n","image = image.to(device)\n","with torch.no_grad():\n","    low_level_features, high_level_features = model(image)\n","\n","# Visualize low-level features\n","#visualize_feature_maps(low_level_features, title=\"Low-Level Feature Maps\")\n","\n","# Visualize high-level features\n","visualize_feature_maps(high_level_features, title=\"High-Level Feature Maps\")"]},{"cell_type":"markdown","metadata":{"id":"9VMkKvnvTqEl"},"source":["visualizing low level and high level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhDPNjb8_9Wn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","class PVTEncoder(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(PVTEncoder, self).__init__()\n","        self.layer1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.layer3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n","        self.layer4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        flow = self.layer1(x)  # Low-level feature\n","        fhigh = self.layer2(flow)\n","        fhigh = self.layer3(x)\n","        #fhigh = self.layer4(x)  # High-level feature\n","        return flow, fhigh\n","\n","# Function to visualize feature maps\n","def visualize_feature_maps(features, title=\"Feature Maps\"):\n","    num_features = min(8, features.shape[1])  # Choose up to 8 feature maps for display\n","    fig, axs = plt.subplots(1, num_features, figsize=(15, 15))\n","    for i in range(num_features):\n","        axs[i].imshow(features[0, i].cpu().detach().numpy(), cmap='viridis')\n","        axs[i].axis('off')\n","    plt.suptitle(title)\n","    plt.show()\n","\n","# Load and preprocess a sample MRI image\n","def load_image(img_path):\n","    image = Image.open(img_path).convert('RGB')\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    image = transform(image).unsqueeze(0)  # Add batch dimension\n","    return image\n","\n","# Load sample image\n","image_path = '/content/drive/MyDrive/Train/Images/159.png'  # Update this path\n","image = load_image(image_path)\n","\n","# Instantiate model and move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = PVTEncoder().to(device)\n","\n","# Pass the sample image through the model\n","image = image.to(device)\n","with torch.no_grad():\n","    low_level_features, high_level_features = model(image)\n","\n","# Visualize low-level features\n","#visualize_feature_maps(low_level_features, title=\"Low-Level Feature Maps\")\n","\n","# Visualize high-level features\n","visualize_feature_maps(high_level_features, title=\"High-Level Feature Maps\")"]},{"cell_type":"markdown","metadata":{"id":"nr_wY7gbT5zJ"},"source":["combination of rvt and efficientnet_b7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVtsytLFwYSz"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision.models import efficientnet_b3  # Use EfficientNet-B3 for deeper feature extraction\n","\n","# EfficientNet Encoder (using EfficientNet-B3 for added complexity)\n","class EfficientNetEncoder(nn.Module):\n","    def __init__(self):\n","        super(EfficientNetEncoder, self).__init__()\n","\n","        efficientnet = efficientnet_b7(pretrained=True)\n","        # Use early layers for low-level features\n","        self.low_level_features = nn.Sequential(*list(efficientnet.features[:1]))  # Adjust as necessary for low-level features\n","\n","        # Use deeper layers for high-level features\n","        self.layer1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.layer3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n","        self.layer4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        # Extract low-level features\n","        flow = self.low_level_features(x)\n","        #print(\"Shape of flow (low-level features):\", flow.shape)\n","\n","        x = self.layer1(x)\n","        fhigh = self.layer2(x)\n","\n","        return flow, fhigh\n","\n","# Feature Fusion Module\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(64, 16, kernel_size=1)  # Adjust channels based on EfficientNet low-level output\n","        self.conv2 = nn.Conv2d(128, 16, kernel_size=1)  # Adjust channels based on EfficientNet high-level output\n","\n","    def forward(self, flow, fhigh):\n","        #print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        #print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        #print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        #print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        # Resize flow_feat to match the spatial dimensions of fhigh_feat\n","        flow_feat = F.interpolate(flow_feat, size=fhigh_feat.shape[2:], mode='bilinear', align_corners=True)\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        #print(\"Shape after concatenation (fused_features):\", fused_features.shape)\n","        return fused_features\n","\n","# Orthogonal Layer\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","# Complete Model\n","class RVTModelWithEfficientNet(nn.Module):\n","    def __init__(self):\n","        super(RVTModelWithEfficientNet, self).__init__()\n","        self.encoder = EfficientNetEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Output shape (32, 8, 8)\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(32 * 8 * 8, 12)  # Changed output to 12 to match classifier input\n","        self.classifier = nn.Linear(12, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        pooled_features = self.pool(fused_features)\n","        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n","        keypoints = self.fc(pooled_features).view(-1, 12)  # Changed to match classifier\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","# Custom Loss Function\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModelWithEfficientNet().to(device)\n","criterion = RVTLoss(gamma=1.0).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","\n","\n","\n","# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"s9yjyc28UCqc"},"source":["using PVT encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPaUoXshwf-w"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# Custom Model Components\n","class PVTEncoder(nn.Module):\n","    def __init__(self, in_channels=3):\n","        super(PVTEncoder, self).__init__()\n","        self.layer1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n","        self.layer2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.layer3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n","        self.layer4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        flow = self.layer1(x)  # Low-level feature\n","        x = self.layer2(flow)\n","        x = self.layer3(x)\n","        fhigh = self.layer4(x)  # High-level feature\n","        return flow, fhigh\n","\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.conv1 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(512, 32, kernel_size=3, padding=1)\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((16, 16))\n","\n","    def forward(self, flow, fhigh):\n","        print(\"Shape of flow before conv1:\", flow.shape)\n","        flow_feat = self.conv1(flow)\n","        print(\"Shape of flow_feat after conv1:\", flow_feat.shape)\n","\n","        print(\"Shape of fhigh before conv2:\", fhigh.shape)\n","        fhigh_feat = self.conv2(fhigh)\n","        print(\"Shape of fhigh_feat after conv2:\", fhigh_feat.shape)\n","\n","        flow_feat = self.adaptive_pool(flow_feat)\n","        fhigh_feat = self.adaptive_pool(fhigh_feat)\n","\n","        if flow_feat.size() != fhigh_feat.size():\n","            fhigh_feat = F.interpolate(fhigh_feat, size=flow_feat.shape[2:], mode='bilinear', align_corners=False)\n","\n","        fused_features = torch.cat((flow_feat, fhigh_feat), dim=1)\n","        return fused_features\n","\n","class OrthogonalLayer(nn.Module):\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4], points[:, 5], points[:, 6], points[:, 7]\n","        s = -(x1 - x2) / (y1 - y2 + 1e-6)\n","        y4_pred = s * (x4 - x3) + y3\n","        y4_corrected = points.clone()\n","        y4_corrected[:, 7] = y4_pred\n","        return y4_corrected\n","\n","class RVTModel(nn.Module):\n","    def __init__(self):\n","        super(RVTModel, self).__init__()\n","        self.encoder = PVTEncoder()\n","        self.ffm = FeatureFusionModule()\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(64 * 16 * 16, 12)\n","        self.classifier = nn.Linear(8, 3)\n","\n","    def forward(self, x):\n","        flow, fhigh = self.encoder(x)\n","        fused_features = self.ffm(flow, fhigh)\n","        print(\"Shape of fusedfeatures1:\", fused_features.shape)\n","        fused_features = fused_features.view(fused_features.size(0), -1)\n","        print(\"Shape of fusedfeatures2:\", fused_features.shape)\n","        keypoints = self.fc(fused_features).view(-1, 8)\n","        keypoints_corrected = self.orthogonal_layer(keypoints)\n","        vhs_class = self.classifier(keypoints_corrected)\n","        return keypoints_corrected, vhs_class\n","\n","class RVTLoss(nn.Module):\n","    def __init__(self, gamma=1.0):\n","        super(RVTLoss, self).__init__()\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.mse_loss = nn.MSELoss()\n","        self.gamma = gamma\n","\n","    def forward(self, predicted_keypoints, true_keypoints, predicted_class, true_class):\n","        print(f\"Shape of predicted_class: {predicted_class.shape}, Shape of true_class: {true_class.shape}\")\n","        print(f\"Shape of predicted_keypoints: {predicted_keypoints.shape}, Shape of true_keypoints: {true_keypoints.shape}\")\n","\n","        l_ce = self.ce_loss(predicted_class, true_class)\n","        l_mse = self.mse_loss(predicted_keypoints, true_keypoints)\n","        loss = l_ce + self.gamma * l_mse\n","        return loss\n","\n","# Instantiate model and loss\n","model = RVTModel()\n","criterion = RVTLoss(gamma=1.0)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training and Evaluation Functions\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_target = labels['keypoints_output'].to(device)\n","        vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","        # Forward pass\n","        keypoints_pred, vhs_class_pred = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    return epoch_loss\n","\n","def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_target = labels['keypoints_output'].to(device)\n","            vhs_class_target = labels['vhs_class_output'].to(device)\n","\n","            # Forward pass\n","            keypoints_pred, vhs_class_pred = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(keypoints_pred, keypoints_target, vhs_class_pred, vhs_class_target)\n","            running_loss += loss.item() * images.size(0)\n","\n","            # Calculate accuracy for VHS class\n","            _, predicted_classes = torch.max(vhs_class_pred, 1)\n","            correct += (predicted_classes == vhs_class_target).sum().item()\n","            total += vhs_class_target.size(0)\n","\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    accuracy = correct / total\n","    return epoch_loss, accuracy\n","\n","# Training Loop\n","num_epochs = 200\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","          f\"Train Loss: {train_loss:.4f} - \"\n","          f\"Validation Loss: {val_loss:.4f} - \"\n","          f\"Validation Accuracy: {val_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kf_Kb0pz7afp"},"outputs":[],"source":["# Check one batch of data in the train_loader\n","for i, (images, labels) in enumerate(train_loader):\n","    if i == 0:  # Only look at the first batch\n","        print(\"Batch #1:\")\n","\n","        # Print image shape (should be [batch_size, channels, height, width])\n","        print(f\"Images shape: {images.shape}\")\n","\n","        # Print the labels (keypoints and class)\n","        print(f\"Keypoints shape: {labels['keypoints_output'].shape}\")\n","        print(f\"VHS class shape: {labels['vhs_class_output'].shape}\")\n","\n","        # Show keypoints and corresponding VHS class for the first image in the batch\n","        print(\"First Image Keypoints:\", labels['keypoints_output'][0].numpy())\n","        print(\"First Image VHS Class:\", labels['vhs_class_output'][0].item())\n","        break  # Exit the loop after inspecting the first batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5f0QXVe4G5r"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","# Step 1: Load a pre-trained ResNet model (ResNet18 here for simplicity)\n","resnet = models.resnet18(pretrained=True)\n","\n","# Modify the last fully connected layer to fit the desired output (feature maps)\n","# ResNet outputs a 512x7x7 feature map, which we will use\n","resnet.fc = nn.Identity()  # We don't need the final classification layer, just the feature maps\n","\n","# Example of how the CNN backbone processes an image\n","class CNNBackbone(nn.Module):\n","    def __init__(self, pretrained_model=resnet):\n","        super(CNNBackbone, self).__init__()\n","        self.cnn = pretrained_model\n","\n","    def forward(self, x):\n","        # Forward pass through the CNN (ResNet in our case)\n","        return self.cnn(x)\n","\n","# Initialize the CNN model\n","cnn_backbone = CNNBackbone()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riemPrDf4R9F"},"outputs":[],"source":["from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class FeatureTransformer(nn.Module):\n","    def __init__(self, feature_size=512, num_heads=8, num_layers=6):\n","        super(FeatureTransformer, self).__init__()\n","\n","        self.flatten = nn.Flatten(start_dim=1)\n","        self.transformer_layer = TransformerEncoderLayer(d_model=feature_size, nhead=num_heads)\n","        self.transformer = TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n","\n","    def forward(self, x):\n","        # Flatten the input feature maps to sequence\n","        x = self.flatten(x)\n","\n","        # Pass through transformer\n","        x = self.transformer(x)\n","        return x\n","\n","# Example: Initialize the transformer block\n","transformer_block = FeatureTransformer()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Bch_Set4Urz"},"outputs":[],"source":["class CNNTransformerModel(nn.Module):\n","    def __init__(self, cnn_backbone, transformer_block, num_keypoints=6):\n","        super(CNNTransformerModel, self).__init__()\n","        self.cnn_backbone = cnn_backbone\n","        self.transformer_block = transformer_block\n","\n","        # Fully connected layers for keypoint prediction (12 coordinates)\n","        self.keypoint_fc = nn.Linear(512, num_keypoints * 2)  # 6 keypoints, 2 coords each\n","\n","        # Fully connected layer for classification (VHS class)\n","        self.classification_fc = nn.Linear(512, 3)  # 3 classes for VHS\n","\n","    def forward(self, x):\n","        # Step 1: Get features from the CNN backbone (ResNet)\n","        cnn_features = self.cnn_backbone(x)  # (batch_size, 512, 7, 7)\n","\n","        # Step 2: Pass through the transformer for global context\n","        transformer_output = self.transformer_block(cnn_features)  # (batch_size, seq_len, feature_size)\n","\n","        # Step 3: Predict the keypoints (12 coordinates)\n","        keypoints = self.keypoint_fc(transformer_output)  # (batch_size, 12)\n","\n","        # Step 4: Predict the VHS class\n","        class_prediction = self.classification_fc(transformer_output.mean(dim=1))  # (batch_size, 3)\n","\n","        return keypoints, class_prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXw8ns6v4aQw"},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def vhs_loss(keypoints_pred, keypoints_true, class_pred, class_true):\n","    # Keypoint loss (MSE)\n","    keypoint_loss = F.mse_loss(keypoints_pred, keypoints_true)\n","\n","    # Classification loss (Cross-Entropy)\n","    classification_loss = F.cross_entropy(class_pred, class_true)\n","\n","    # Total loss\n","    total_loss = keypoint_loss + classification_loss\n","    return total_loss\n","\n","# Example: Optimizer\n","model = CNNTransformerModel(cnn_backbone, transformer_block)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgzfiR0L4b7K"},"outputs":[],"source":["import torch\n","\n","def compute_vhs(keypoints):\n","    # Compute distances between keypoints\n","    AB = torch.dist(keypoints[:, 0], keypoints[:, 1])  # AB distance\n","    CD = torch.dist(keypoints[:, 2], keypoints[:, 3])  # CD distance\n","    EF = torch.dist(keypoints[:, 4], keypoints[:, 5])  # EF distance\n","\n","    # Compute VHS\n","    vhs = 6 * (AB + CD) / EF\n","    return vhs\n","\n","def compute_class(vhs_values):\n","    # Compute class based on VHS value\n","    vhs_class = torch.where(vhs_values < 8.2, torch.tensor(0),\n","                            torch.where(vhs_values <= 10, torch.tensor(1), torch.tensor(2)))\n","    return vhs_class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oUwEBaa4gNB"},"outputs":[],"source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    for data in train_loader:\n","        images, labels = data  # Images and labels dictionary\n","\n","        # Access keypoints and class from the labels dictionary\n","        keypoints_true = labels['keypoints_output']\n","        class_true = labels['vhs_class_output']\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        keypoints_pred, class_pred = model(images)\n","\n","        # Compute loss\n","        loss = vhs_loss(keypoints_pred, keypoints_true, class_pred, class_true)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss / len(train_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3XSiOY_4mMV"},"outputs":[],"source":["def evaluate(model, val_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, keypoints_true, class_true = data\n","\n","            keypoints_pred, class_pred = model(images)\n","\n","            # Compute VHS and class prediction\n","            vhs_values = compute_vhs(keypoints_pred)\n","            predicted_class = compute_class(vhs_values)\n","\n","            # Compute accuracy\n","            correct += (predicted_class == class_true).sum().item()\n","            total += class_true.size(0)\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1ZbOoWN5g-V"},"outputs":[],"source":["# Train and evaluate for a number of epochs\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","\n","    # Train the model\n","    train_loss = train(model, train_loader, optimizer)\n","    print(f\"Training Loss: {train_loss:.4f}\")\n","\n","    # Evaluate the model\n","    val_loss, val_accuracy = evaluate(model, val_loader)\n","    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBOs9QSuAo_I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","# Step 1: Load a pre-trained ResNet model (ResNet18 here for simplicity)\n","resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # Updated for the latest API\n","\n","# Modify the last fully connected layer to fit the desired output (feature maps)\n","resnet.fc = nn.Identity()  # We don't need the final classification layer, just the feature maps\n","\n","# Example of how the CNN backbone processes an image\n","class CNNBackbone(nn.Module):\n","    def __init__(self, pretrained_model=resnet):\n","        super(CNNBackbone, self).__init__()\n","        self.cnn = pretrained_model\n","\n","    def forward(self, x):\n","        # Forward pass through the CNN (ResNet in our case)\n","        return self.cnn(x)\n","\n","# Initialize the CNN model\n","cnn_backbone = CNNBackbone()\n","\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class FeatureTransformer(nn.Module):\n","    def __init__(self, feature_size=512, num_heads=8, num_layers=6):\n","        super(FeatureTransformer, self).__init__()\n","\n","        self.flatten = nn.Flatten(start_dim=2)  # This will work if tensor has shape (batch_size, 512, 7, 7)\n","        self.transformer_layer = TransformerEncoderLayer(d_model=feature_size, nhead=num_heads)\n","        self.transformer = TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n","\n","    def forward(self, x):\n","        # Flatten the input feature maps (batch_size, 512, 7, 7) -> (batch_size, 49, 512)\n","        x = x.view(x.size(0), 512, -1).transpose(1, 2)  # Reshape to (batch_size, 49, 512)\n","\n","        # Pass through transformer\n","        x = self.transformer(x)\n","        return x\n","\n","# Example: Initialize the transformer block\n","transformer_block = FeatureTransformer()\n","\n","class CNNTransformerModel(nn.Module):\n","    def __init__(self, cnn_backbone, transformer_block, num_keypoints=6):\n","        super(CNNTransformerModel, self).__init__()\n","        self.cnn_backbone = cnn_backbone\n","        self.transformer_block = transformer_block\n","\n","        # Fully connected layers for keypoint prediction (12 coordinates)\n","        self.keypoint_fc = nn.Linear(512, num_keypoints * 2)  # 6 keypoints, 2 coords each\n","\n","        # Fully connected layer for classification (VHS class)\n","        self.classification_fc = nn.Linear(512, 3)  # 3 classes for VHS\n","\n","    def forward(self, x):\n","        # Step 1: Get features from the CNN backbone (ResNet)\n","        cnn_features = self.cnn_backbone(x)  # (batch_size, 512, 7, 7)\n","\n","        # Step 2: Pass through the transformer for global context\n","        transformer_output = self.transformer_block(cnn_features)  # (batch_size, seq_len, feature_size)\n","\n","        # Step 3: Predict the keypoints (12 coordinates)\n","        # Use the mean of the transformer output for keypoint prediction\n","        keypoints = self.keypoint_fc(transformer_output.mean(dim=1))  # (batch_size, 12)\n","\n","        # Step 4: Predict the VHS class\n","        class_prediction = self.classification_fc(transformer_output.mean(dim=1))  # (batch_size, 3)\n","\n","        return keypoints, class_prediction\n","\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def vhs_loss(keypoints_pred, keypoints_true, class_pred, class_true):\n","    # Keypoint loss (MSE)\n","    keypoint_loss = F.mse_loss(keypoints_pred, keypoints_true)\n","\n","    # Classification loss (Cross-Entropy)\n","    classification_loss = F.cross_entropy(class_pred, class_true)\n","\n","    # Total loss\n","    total_loss = keypoint_loss + classification_loss\n","    return total_loss\n","\n","# Example: Optimizer\n","model = CNNTransformerModel(cnn_backbone, transformer_block)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","import torch\n","\n","def compute_vhs(keypoints):\n","    # Compute distances between keypoints\n","    AB = torch.dist(keypoints[:, 0], keypoints[:, 1])  # AB distance\n","    CD = torch.dist(keypoints[:, 2], keypoints[:, 3])  # CD distance\n","    EF = torch.dist(keypoints[:, 4], keypoints[:, 5])  # EF distance\n","\n","    # Compute VHS\n","    vhs = 6 * (AB + CD) / EF\n","    return vhs\n","\n","def compute_class(vhs_values):\n","    # Compute class based on VHS value\n","    vhs_class = torch.where(vhs_values < 8.2, torch.tensor(0),\n","                            torch.where(vhs_values <= 10, torch.tensor(1), torch.tensor(2)))\n","    return vhs_class\n","\n","def train(model, train_loader, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    for data in train_loader:\n","        images, labels = data  # Images and labels dictionary\n","\n","        # Access keypoints and class from the labels dictionary\n","        keypoints_true = labels['keypoints_output']\n","        class_true = labels['vhs_class_output']\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        keypoints_pred, class_pred = model(images)\n","\n","        # Compute loss\n","        loss = vhs_loss(keypoints_pred, keypoints_true, class_pred, class_true)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss / len(train_loader)\n","\n","def evaluate(model, val_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in val_loader:\n","            images, labels = data  # Images and labels dictionary\n","\n","            # Access keypoints and class from the labels dictionary\n","            keypoints_true = labels['keypoints_output']\n","            class_true = labels['vhs_class_output']\n","\n","            keypoints_pred, class_pred = model(images)\n","\n","            # Compute VHS and class prediction\n","            vhs_values = compute_vhs(keypoints_pred)\n","            predicted_class = compute_class(vhs_values)\n","\n","            # Compute accuracy\n","            correct += (predicted_class == class_true).sum().item()\n","            total += class_true.size(0)\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","# Train and evaluate for a number of epochs\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","\n","    # Train the model\n","    train_loss = train(model, train_loader, optimizer)\n","    print(f\"Training Loss: {train_loss:.4f}\")\n","\n","    # Evaluate the model\n","    val_accuracy = evaluate(model, val_loader)\n","    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UbkZUkL3QdJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SRAAttention(nn.Module):\n","    def __init__(self, Ci, Ri, dhead):\n","        super(SRAAttention, self).__init__()\n","        self.Ri = Ri\n","        self.WQ = nn.Linear(10752, 256)\n","        self.WK = nn.Linear(10752, 256)\n","        self.WV = nn.Linear(10752, 256)\n","        self.WO = nn.Linear(dhead, Ci)\n","\n","    def forward(self, x):\n","        print(\"Shape before WQ:\", x.shape)\n","        Q = self.WQ(x)\n","\n","        K = self.WK(x)\n","        V = self.WV(x)\n","\n","        # Apply Spatial Reduction\n","        K = self.spatial_reduction(K)\n","        V = self.spatial_reduction(V)\n","\n","        # Attention calculation\n","        attention = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        head = torch.matmul(attention, V)\n","        output = self.WO(head)\n","        return output\n","\n","    def spatial_reduction(self, x):\n","        # Apply spatial reduction based on Ri\n","        return x.view(x.size(0), self.Ri, -1).mean(dim=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPlgdgtR3RjP"},"outputs":[],"source":["class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.Conv1 = nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(1, 16), stride=96, padding=1)\n","        self.Conv2 = nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(1, 16), stride=16, dilation=9, padding=1)\n","\n","    def forward(self, low_level_features, high_level_features):\n","        f1 = self.Conv1(low_level_features)\n","        f2 = self.Conv2(high_level_features)\n","        fused_features = f1 * f2  # Element-wise multiplication for feature fusion\n","        return fused_features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJus5ecF3UD9"},"outputs":[],"source":["class OrthogonalLayer(nn.Module):\n","    def __init__(self):\n","        super(OrthogonalLayer, self).__init__()\n","\n","    def forward(self, points):\n","        # Given points (A, B, C, D), ensure perpendicularity of AB and CD.\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points\n","        # Calculate slope of line CD\n","        s = (y4 - y3) / (x4 - x3)\n","        # Replace y4 based on the slope\n","        y4_new = y1 + s * (x4 - x1)\n","        return torch.cat([points[:7], y4_new.unsqueeze(1)], dim=-1)  # Update y4 to ensure perpendicularity\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnxALhAj3WxS"},"outputs":[],"source":["class RVTModel(nn.Module):\n","    def __init__(self):\n","        super(RVTModel, self).__init__()\n","        # Define layers (e.g., PVT transformer, FFM, Orthogonal layer)\n","        self.pvt_transformer = SRAAttention(Ci=256, Ri=8, dhead=64)  # Example configuration\n","        self.ffm = FeatureFusionModule()\n","        self.orthogonal_layer = OrthogonalLayer()\n","        self.fc = nn.Linear(256, 12)  # Final fully connected layer for 6 keypoints\n","\n","    def forward(self, x):\n","        # Extract low-level and high-level features using PVT\n","        low_level_features, high_level_features = self.pvt_transformer(x)\n","\n","        # Fuse features using FFM\n","        fused_features = self.ffm(low_level_features, high_level_features)\n","\n","        # Predict keypoints\n","        keypoints = self.fc(fused_features)\n","\n","        # Apply orthogonal layer\n","        keypoints = self.orthogonal_layer(keypoints)\n","        return keypoints\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWZO7veF3enw"},"outputs":[],"source":["def loss_function(pred_keypoints, true_keypoints, vhs_class_pred, vhs_class_true, gamma=1.0):\n","    # MSE loss for keypoints\n","    mse_loss = F.mse_loss(pred_keypoints, true_keypoints)\n","\n","    # Cross-entropy loss for VHS classification\n","    ce_loss = F.cross_entropy(vhs_class_pred, vhs_class_true)\n","\n","    # Total loss\n","    total_loss = ce_loss + gamma * mse_loss\n","    return total_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjUXYa-L3ka5"},"outputs":[],"source":["def train(model, train_loader, optimizer, gamma=1.0):\n","    model.train()\n","    total_loss = 0\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_true = labels['keypoints_output'].to(device)\n","        vhs_class_true = labels['vhs_class_output'].to(device)\n","\n","        optimizer.zero_grad()\n","        keypoints_pred = model(images)\n","\n","        loss = loss_function(keypoints_pred, keypoints_true, vhs_class_pred, vhs_class_true, gamma)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(train_loader)\n","\n","def evaluate(model, val_loader):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","    with torch.no_grad():  # Disable gradient computation\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_true = labels['keypoints_output'].to(device)\n","            vhs_class_true = labels['vhs_class_output'].to(device)\n","\n","            # Make predictions\n","            keypoints_pred = model(images)\n","\n","            # Calculate the loss\n","            loss = loss_function(keypoints_pred, keypoints_true, vhs_class_pred, vhs_class_true)\n","            total_loss += loss.item()\n","\n","            # Calculate classification accuracy (for VHS classification)\n","            _, predicted_class = vhs_class_pred.max(1)  # Get the class with the highest probability\n","            total_correct += (predicted_class == vhs_class_true).sum().item()\n","            total_samples += vhs_class_true.size(0)\n","\n","    # Calculate average loss\n","    avg_loss = total_loss / len(val_loader)\n","    # Calculate accuracy for VHS classification\n","    accuracy = 100 * total_correct / total_samples\n","\n","    return avg_loss, accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BigIHzaE3qrg"},"outputs":[],"source":["# Check if CUDA (GPU) is available, otherwise use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate the model and optimizer\n","model = RVTModel().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    train_loss = train(model, train_loader, optimizer)\n","    val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate on validation set\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik7Pt5_FJH3K"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SRAAttention(nn.Module):\n","    def __init__(self, flattened_size, dhead):\n","        super(SRAAttention, self).__init__()\n","        self.WQ = nn.Linear(flattened_size, dhead)\n","        self.WK = nn.Linear(flattened_size, dhead)\n","        self.WV = nn.Linear(flattened_size, dhead)\n","        self.WO = nn.Linear(dhead, flattened_size)\n","\n","    def forward(self, x):\n","        B, C, H, W = x.size()\n","        x = x.view(B, C, H * W)  # Flatten spatial dimensions (height, width)\n","        Q = self.WQ(x)\n","        K = self.WK(x)\n","        V = self.WV(x)\n","\n","        attention = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n","        attention = F.softmax(attention, dim=-1)\n","        head = torch.matmul(attention, V)\n","\n","        output = self.WO(head)\n","        return output.view(B, C, H, W)  # Reshape back to original spatial dimensions\n","\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self):\n","        super(FeatureFusionModule, self).__init__()\n","        self.Conv1 = nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(1, 16), stride=1, padding=1)\n","        self.Conv2 = nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(1, 16), stride=1, padding=1)\n","\n","    def forward(self, low_level_features, high_level_features):\n","        f1 = self.Conv1(low_level_features)\n","        f2 = self.Conv2(high_level_features)\n","        fused_features = f1 * f2  # Element-wise multiplication for feature fusion\n","        return fused_features\n","\n","class OrthogonalLayer(nn.Module):\n","    def __init__(self):\n","        super(OrthogonalLayer, self).__init__()\n","\n","    def forward(self, points):\n","        x1, y1, x2, y2, x3, y3, x4, y4 = points\n","        s = (y4 - y3) / (x4 - x3)\n","        y4_new = y1 + s * (x4 - x1)\n","        return torch.cat([points[:7], y4_new.unsqueeze(1)], dim=-1)\n","\n","class RVTModel(nn.Module):\n","    def __init__(self):\n","        super(RVTModel, self).__init__()\n","        self.input_layer = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.AvgPool2d(kernel_size=8, stride=8)  # Reduce dimensions before attention\n","        self.flattened_size = 256 * 28 * 28  # Calculate flattened size after pooling\n","        self.pvt_transformer = SRAAttention(flattened_size=self.flattened_size, dhead=256)\n","        self.ffm = FeatureFusionModule()\n","        self.orthogonal_layer = OrthogonalLayer()\n","\n","        # Update fully connected layer to match the flattened size after pooling\n","        self.fc = nn.Linear(self.flattened_size, 12)  # Output layer size\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        print(\"Shape after input_layer:\", x.shape)  # Debug print\n","        x = self.pool(x)\n","        print(\"Shape after pooling:\", x.shape)  # Debug print\n","\n","        high_level_features = self.pvt_transformer(x)\n","        print(\"Shape after attention:\", high_level_features.shape)  # Debug print\n","        low_level_features = F.interpolate(x, scale_factor=0.5)\n","        print(\"Shape of low_level_features:\", low_level_features.shape)  # Debug print\n","\n","        fused_features = self.ffm(low_level_features, high_level_features)\n","        print(\"Shape after feature fusion:\", fused_features.shape)  # Debug print\n","        fused_features = fused_features.view(fused_features.size(0), -1)  # Flatten for fc layer\n","        print(\"Shape after flattening for fc:\", fused_features.shape)  # Debug print\n","        keypoints = self.fc(fused_features)\n","\n","        keypoints = self.orthogonal_layer(keypoints)\n","        return keypoints\n","\n","def loss_function(pred_keypoints, true_keypoints, vhs_class_pred, vhs_class_true, gamma=1.0):\n","    mse_loss = F.mse_loss(pred_keypoints, true_keypoints)\n","    ce_loss = F.cross_entropy(vhs_class_pred, vhs_class_true)\n","    total_loss = ce_loss + gamma * mse_loss\n","    return total_loss\n","\n","def train(model, train_loader, optimizer, gamma=1.0):\n","    model.train()\n","    total_loss = 0\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        keypoints_true = labels['keypoints_output'].to(device)\n","        vhs_class_true = labels['vhs_class_output'].to(device)\n","\n","        optimizer.zero_grad()\n","        keypoints_pred = model(images)\n","\n","        loss = loss_function(keypoints_pred, keypoints_true, vhs_class_pred, vhs_class_true, gamma)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    return total_loss / len(train_loader)\n","\n","def evaluate(model, val_loader):\n","    model.eval()\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            keypoints_true = labels['keypoints_output'].to(device)\n","            vhs_class_true = labels['vhs_class_output'].to(device)\n","\n","            keypoints_pred = model(images)\n","            loss = loss_function(keypoints_pred, keypoints_true, vhs_class_pred, vhs_class_true)\n","            total_loss += loss.item()\n","\n","            _, predicted_class = vhs_class_pred.max(1)\n","            total_correct += (predicted_class == vhs_class_true).sum().item()\n","            total_samples += vhs_class_true.size(0)\n","\n","    avg_loss = total_loss / len(val_loader)\n","    accuracy = 100 * total_correct / total_samples\n","    return avg_loss, accuracy\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = RVTModel().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = train(model, train_loader, optimizer)\n","    val_loss, val_accuracy = evaluate(model, val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sR45O0Js2Ztw"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.cuda.amp as amp  # For mixed precision\n","from pathlib import Path\n","import sys\n","sys.path.append('./yolov7')  # Add yolov7 directory to the path\n","\n","# Import YOLOv7 model components\n","from models.experimental import attempt_load  # YOLOv7 model loader\n","\n","# Define the model class\n","class KeypointVHS_Model(nn.Module):\n","    def __init__(self, num_keypoints=6, num_classes=3, device='cuda'):\n","        super(KeypointVHS_Model, self).__init__()\n","\n","        # Load YOLOv7 as the first stage (feature extractor)\n","        yolov7_weights_path = '/content/drive/MyDrive/YOLOv7/yolov7.pt'\n","\n","        if Path(yolov7_weights_path).exists():\n","            yolov7_model = torch.load(yolov7_weights_path, map_location=device)  # Load the model weights directly\n","            self.yolov7 = yolov7_model['model'] if 'model' in yolov7_model else yolov7_model  # Handle model structure\n","        else:\n","            raise FileNotFoundError(f\"YOLOv7 weights file not found at {yolov7_weights_path}\")\n","\n","        self.yolov7.eval()  # Set YOLOv7 to eval mode for feature extraction only\n","\n","        # Freeze YOLOv7 weights initially\n","        for param in self.yolov7.parameters():\n","            param.requires_grad = False\n","\n","        # Define output heads for keypoints and VHS classification\n","        self.keypoint_head = nn.Linear(1024, num_keypoints * 2).to(device)  # 12 values for 6 points\n","        self.class_head = nn.Linear(1024, num_classes).to(device)\n","\n","    def forward(self, x):\n","        x = x.to(torch.float32)  # Convert input to float32\n","\n","        with torch.no_grad():  # Forward pass through YOLOv7 (first stage)\n","            yolov7_features = self.yolov7(x)[0]  # Get features from YOLOv7\n","\n","        yolov7_features = [yolov7_features]  # Convert to list\n","        pooled_features = yolov7_features[0].mean(dim=1)  # Global average pooling\n","\n","        keypoints = self.keypoint_head(pooled_features)\n","        vhs_class = self.class_head(pooled_features)\n","\n","        return keypoints, vhs_class\n","\n","\n","# Loss functions\n","criterion_keypoints = nn.MSELoss()  # For keypoint regression\n","criterion_class = nn.CrossEntropyLoss()  # For VHS class classification\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = KeypointVHS_Model(device=device)  # Ensure to initialize with device\n","model.to(device)\n","\n","# Optimizer - Move this after model initialization\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n","\n","def train_model(model, train_loader, val_loader, epochs=150, unfreeze_epoch=100):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_class = 0\n","        total_samples = 0\n","\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            keypoints = labels['keypoints_output'].to(device)\n","            vhs_classes = labels['vhs_class_output'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass with mixed precision\n","            with amp.autocast('cuda'):  # Mixed precision context (with device specified)\n","                pred_keypoints, pred_class = model(images)  # Use model instance here, not class\n","\n","                # Calculate losses\n","                loss_keypoints = criterion_keypoints(pred_keypoints, keypoints)\n","                loss_class = criterion_class(pred_class, vhs_classes)\n","                loss = loss_keypoints + loss_class  # Total loss\n","\n","            # Backward pass and optimization with gradient scaling\n","            scaler.scale(loss).backward()  # Backprop with scaled gradients\n","            scaler.step(optimizer)  # Optimizer step\n","            scaler.update()  # Update the scaler\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, predicted_classes = torch.max(pred_class, 1)\n","            correct_class += (predicted_classes == vhs_classes).sum().item()\n","            total_samples += vhs_classes.size(0)\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_class / total_samples\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","        if epoch == unfreeze_epoch - 1:\n","            for param in model.yolov7.parameters():\n","                param.requires_grad = True  # Unfreeze YOLOv7 layers\n","            print(\"Unfreezing YOLOv7 backbone layers.\")\n","\n","    print(\"Training Complete\")\n","\n","train_model(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0-mktYytKbV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.cuda.amp as amp  # For mixed precision\n","from pathlib import Path\n","import sys\n","sys.path.append('./yolov7')  # Add yolov7 directory to the path\n","\n","# Import YOLOv7 model components\n","from models.experimental import attempt_load  # YOLOv7 model loader\n","\n","# Define the model class\n","class KeypointVHS_Model(nn.Module):\n","    def __init__(self, num_keypoints=6, num_classes=3, device='cuda'):\n","        super(KeypointVHS_Model, self).__init__()\n","\n","        # Load YOLOv7 as the first stage (feature extractor)\n","        yolov7_weights_path = '/content/drive/MyDrive/YOLOv7/yolov7.pt'\n","\n","        if Path(yolov7_weights_path).exists():\n","            yolov7_model = torch.load(yolov7_weights_path, map_location=device)  # Load the model weights directly\n","            self.yolov7 = yolov7_model['model'] if 'model' in yolov7_model else yolov7_model  # Handle model structure\n","        else:\n","            raise FileNotFoundError(f\"YOLOv7 weights file not found at {yolov7_weights_path}\")\n","\n","        self.yolov7.eval()  # Set YOLOv7 to eval mode for feature extraction only\n","\n","        # Freeze YOLOv7 weights initially\n","        for param in self.yolov7.parameters():\n","            param.requires_grad = False\n","\n","        # Define output heads for keypoints and VHS classification\n","        self.keypoint_head = nn.Linear(1024, num_keypoints * 2).to(device)  # 12 values for 6 points\n","        self.class_head = nn.Linear(1024, num_classes).to(device)\n","\n","    def forward(self, x):\n","        x = x.to(torch.float32)  # Convert input to float32\n","\n","        print(x.shape)\n","\n","        with torch.no_grad():  # Forward pass through YOLOv7 (first stage)\n","            yolov7_features = self.yolov7(x)[0]  # Get features from YOLOv7\n","\n","        yolov7_features = [yolov7_features]  # Convert to list\n","        pooled_features = yolov7_features[0].mean(dim=1)  # Global average pooling\n","\n","        keypoints = self.keypoint_head(pooled_features)\n","        vhs_class = self.class_head(pooled_features)\n","\n","        return keypoints, vhs_class\n","\n","\n","# Loss functions\n","criterion_keypoints = nn.MSELoss()  # For keypoint regression\n","criterion_class = nn.CrossEntropyLoss()  # For VHS class classification\n","\n","# Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Training loop\n","def train_model(model, train_loader, val_loader, epochs=150, unfreeze_epoch=100):\n","    model.train()\n","    scaler = amp.GradScaler()  # Gradient scaler for mixed precision\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_class = 0\n","        total_samples = 0\n","\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            keypoints = labels['keypoints_output'].to(device)\n","            vhs_classes = labels['vhs_class_output'].to(device)\n","            print(keypoints)\n","            print(vhs_classes)\n","            optimizer.zero_grad()\n","\n","            # Forward pass with mixed precision\n","            with amp.autocast(device_type='cuda'):  # Mixed precision context (with device specified)\n","                print('sara is here')\n","                pred_keypoints, pred_class = model(images)  # Use model instance here, not class\n","                print('sara is:', pred_keypoints)\n","                print(pred_class)\n","                # Calculate losses\n","                loss_keypoints = criterion_keypoints(pred_keypoints, keypoints)\n","                loss_class = criterion_class(pred_class, vhs_classes)\n","                loss = loss_keypoints + loss_class  # Total loss\n","\n","            # Backward pass and optimization with gradient scaling\n","            scaler.scale(loss).backward()  # Backprop with scaled gradients\n","            scaler.step(optimizer)  # Optimizer step\n","            scaler.update()  # Update the scaler\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, predicted_classes = torch.max(pred_class, 1)\n","            correct_class += (predicted_classes == vhs_classes).sum().item()\n","            total_samples += vhs_classes.size(0)\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_class / total_samples\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","        if epoch == unfreeze_epoch - 1:\n","            for param in model.yolov7.parameters():\n","                param.requires_grad = True  # Unfreeze YOLOv7 layers\n","            print(\"Unfreezing YOLOv7 backbone layers.\")\n","\n","    print(\"Training Complete\")\n","\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = KeypointVHS_Model(device=device)  # Ensure to initialize with device\n","model.to(device)\n","scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n","\n","train_model(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtDcnIoTOFrT"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from pathlib import Path\n","import sys\n","sys.path.append('./yolov7')  # Add yolov7 directory to the path\n","\n","# Import YOLOv7 model components\n","from models.experimental import attempt_load  # YOLOv7 model loader\n","\n","class MultiStageTransformerModel(nn.Module):\n","    def __init__(self, num_keypoints=6, num_classes=3):\n","        super(MultiStageTransformerModel, self).__init__()\n","\n","        # Load YOLOv7 as the first transformer stage (feature extractor)\n","        yolov7_weights_path = '/content/drive/MyDrive/YOLOv7/yolov7.pt'\n","\n","        # Directly load the YOLOv7 model using torch.load to avoid downloading\n","        if Path(yolov7_weights_path).exists():\n","            yolov7_model = torch.load(yolov7_weights_path, map_location='cpu')  # Load the model weights directly\n","            self.yolov7 = yolov7_model['model'] if 'model' in yolov7_model else yolov7_model  # Handle model structure\n","        else:\n","            raise FileNotFoundError(f\"YOLOv7 weights file not found at {yolov7_weights_path}\")\n","\n","        self.yolov7.eval()  # Set YOLOv7 to eval mode for feature extraction only\n","\n","        # Freeze YOLOv7 weights initially\n","        for param in self.yolov7.parameters():\n","            param.requires_grad = False\n","\n","        # Load Faster R-CNN as the second transformer stage\n","        self.faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","        # Set hidden size based on the Faster R-CNN output size\n","        self.hidden_size = 1024  # Adjust based on Faster R-CNN’s output\n","\n","        # Define output heads for keypoints and VHS classification\n","        self.keypoint_head = nn.Linear(self.hidden_size, num_keypoints * 2)  # 12 values for 6 points\n","        self.class_head = nn.Linear(self.hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        # Ensure input is in float32 for compatibility with mixed precision\n","        x = x.to(torch.float32)  # Convert input to float32\n","\n","        with torch.no_grad():  # Forward pass through YOLOv7 (first stage)\n","            yolov7_features = self.yolov7(x)[0]  # Run YOLOv7 and get first output (for feature maps)\n","\n","        # Forward pass through Faster R-CNN (second stage) using YOLOv7 features as input\n","        # Make sure the input is in the right format for Faster R-CNN (list of images)\n","        yolov7_features = [yolov7_features]  # Convert to list\n","        faster_rcnn_features = self.faster_rcnn(yolov7_features)  # Pass through Faster R-CNN\n","\n","        # Use pooled features for final predictions\n","        pooled_features = faster_rcnn_features[0].mean(dim=1)  # Global average pooling (assuming feature map from Faster R-CNN)\n","\n","        # Keypoint and class predictions\n","        keypoints = self.keypoint_head(pooled_features)\n","        vhs_class = self.class_head(pooled_features)\n","\n","        return keypoints, vhs_class\n","\n","# Instantiate the model\n","model = MultiStageTransformerModel(num_keypoints=6, num_classes=3)\n","\n","# Define loss functions for both outputs\n","criterion_keypoints = nn.MSELoss()  # Mean Squared Error for keypoint regression\n","criterion_class = nn.CrossEntropyLoss()  # Cross-Entropy for classification\n","\n","# Define optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Data augmentation transformation (for PIL Images or ndarrays)\n","data_augmentation = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","])\n","\n","# Define training loop\n","def train_model(model, train_loader, val_loader, epochs=150, unfreeze_epoch=100):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_class = 0\n","        total_samples = 0\n","\n","        for images, labels in train_loader:\n","            # Apply data augmentation if images are PIL Images or tensors\n","            if isinstance(images, torch.Tensor):\n","                # Skip transformation if already a tensor\n","                images = data_augmentation(images)\n","\n","            images = images.to(device)\n","            keypoints = labels['keypoints_output'].to(device)\n","            vhs_classes = labels['vhs_class_output'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Prepare targets for Faster R-CNN\n","            targets = []\n","            for i in range(len(images)):\n","                target = {}\n","                target['boxes'] = labels['keypoints_output'][i].to(device)  # bounding boxes\n","                target['labels'] = labels['vhs_class_output'][i].to(device)  # labels (class ids)\n","                targets.append(target)\n","\n","            # Forward pass with autocast for mixed precision\n","            with torch.cuda.amp.autocast():  # Mixed precision context\n","                pred_keypoints, pred_class = model(images)\n","\n","                # Calculate losses\n","                loss_keypoints = criterion_keypoints(pred_keypoints, keypoints)\n","                loss_class = criterion_class(pred_class, vhs_classes)\n","                loss = loss_keypoints + loss_class  # Combine losses\n","\n","            # Backward pass and optimization with gradient scaling\n","            scaler.scale(loss).backward()  # Scale the loss before backward pass\n","            scaler.step(optimizer)  # Step the optimizer with scaled gradients\n","            scaler.update()  # Update the scaler after stepping the optimizer\n","\n","            # Track metrics\n","            running_loss += loss.item() * images.size(0)\n","            _, predicted_classes = torch.max(pred_class, 1)\n","            correct_class += (predicted_classes == vhs_classes).sum().item()\n","            total_samples += vhs_classes.size(0)\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_class / total_samples\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","        # Unfreeze YOLOv7 layers after the specified epoch\n","        if epoch == unfreeze_epoch - 1:  # After unfreeze_epoch epochs\n","            for param in model.yolov7.parameters():\n","                param.requires_grad = True  # Unfreeze all layers in YOLOv7\n","            print(\"Unfreezing YOLOv7 backbone layers.\")\n","\n","    print(\"Training Complete\")\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","scaler = torch.cuda.amp.GradScaler()  # Gradient scaler for mixed precision\n","\n","train_model(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVNWWnf72XaZ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from transformers import SwinForImageClassification\n","\n","class MultiOutputModel(nn.Module):\n","    def __init__(self, num_keypoints=6, num_classes=3):\n","        super(MultiOutputModel, self).__init__()\n","\n","        # Load Swin Transformer as backbone (without classification head)\n","        self.backbone = SwinForImageClassification.from_pretrained(\n","            \"microsoft/swin-base-patch4-window7-224\",\n","            num_labels=0  # Remove head\n","        )\n","        self.backbone.config.output_hidden_states = True\n","\n","        # Freeze transformer weights initially\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","\n","        # Set the output size from the backbone\n","        self.hidden_size = self.backbone.config.hidden_size\n","        self.keypoint_head = nn.Linear(self.hidden_size, num_keypoints * 2)  # 12 values for 6 points\n","        self.class_head = nn.Linear(self.hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        outputs = self.backbone(x)  # Forward pass through the backbone\n","\n","        # Get hidden states\n","        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n","\n","        # Use global average pooling\n","        pooled_features = hidden_states.mean(dim=1)  # Average over the sequence length (49 in your case)\n","\n","        # Predict keypoints (regression) and class (classification)\n","        keypoints = self.keypoint_head(pooled_features)\n","        vhs_class = self.class_head(pooled_features)  # Use pooled features for class prediction\n","\n","        return keypoints, vhs_class\n","\n","# Instantiate the model\n","model = MultiOutputModel(num_keypoints=6, num_classes=3)\n","\n","# Define loss functions for both outputs\n","criterion_keypoints = nn.MSELoss()  # Mean Squared Error for keypoint regression\n","criterion_class = nn.CrossEntropyLoss()  # Cross-Entropy for classification\n","\n","# Define optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Data augmentation transformation (for PIL Images or ndarrays)\n","data_augmentation = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","])\n","\n","# Define training loop\n","def train_model(model, train_loader, val_loader, epochs=150, unfreeze_epoch=100):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_class = 0\n","        total_samples = 0\n","\n","        for images, labels in train_loader:\n","            # If images are PIL Images, apply data augmentation\n","            if isinstance(images, torch.Tensor):\n","                # Skip transformation if already a tensor\n","                images = data_augmentation(images)\n","\n","            images = images.to(device)\n","            keypoints = labels['keypoints_output'].to(device)\n","            vhs_classes = labels['vhs_class_output'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            pred_keypoints, pred_class = model(images)\n","\n","            # Calculate losses\n","            loss_keypoints = criterion_keypoints(pred_keypoints, keypoints)\n","            loss_class = criterion_class(pred_class, vhs_classes)\n","            loss = loss_keypoints + loss_class  # Combine losses\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Track metrics\n","            running_loss += loss.item() * images.size(0)\n","            _, predicted_classes = torch.max(pred_class, 1)\n","            correct_class += (predicted_classes == vhs_classes).sum().item()\n","            total_samples += vhs_classes.size(0)\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_class / total_samples\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","        # Unfreeze some layers after the specified epoch\n","        if epoch == unfreeze_epoch - 1:  # After unfreeze_epoch epochs\n","            for param in model.backbone.parameters():\n","                param.requires_grad = True  # Unfreeze all layers\n","            print(\"Unfreezing Swin Transformer backbone layers.\")\n","\n","    print(\"Training Complete\")\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","train_model(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBz61YJC2HZp"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from transformers import SwinForImageClassification\n","\n","class MultiOutputModel(nn.Module):\n","    def __init__(self, num_keypoints=6, num_classes=3):\n","        super(MultiOutputModel, self).__init__()\n","\n","        # Load Swin Transformer as backbone (without classification head)\n","        self.backbone = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224\",\n","                                                                   num_labels=0)  # Remove head\n","        self.backbone.config.output_hidden_states = True\n","\n","        # Freeze transformer weights initially\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","\n","        # Set the output size from the backbone\n","        self.hidden_size = self.backbone.config.hidden_size\n","        self.keypoint_head = nn.Linear(self.hidden_size, num_keypoints * 2)  # 12 values for 6 points\n","        self.class_head = nn.Linear(self.hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        outputs = self.backbone(x)  # Forward pass through the backbone\n","\n","        # Get hidden states\n","        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n","\n","        # Debugging shapes\n","        #print(f\"Hidden States Shape: {hidden_states.shape}\")  # Shape: (batch_size, num_layers, height, width)\n","\n","        # Use global average pooling\n","        pooled_features = hidden_states.mean(dim=1)  # Average over the sequence length (49 in your case)\n","\n","        # Debugging pooled features shape\n","        #print(f\"Pooled Features Shape: {pooled_features.shape}\")  # Shape: (batch_size, hidden_size)\n","\n","        # Predict keypoints (regression) and class (classification)\n","        keypoints = self.keypoint_head(pooled_features)\n","        vhs_class = self.class_head(pooled_features)  # Use pooled features for class prediction\n","\n","        # Debugging output shapes\n","        #print(f\"Keypoints Shape: {keypoints.shape}\")  # Should be (batch_size, num_keypoints * 2)\n","        #print(f\"Class Predictions Shape: {vhs_class.shape}\")  # Should be (batch_size, num_classes)\n","\n","        return keypoints, vhs_class\n","\n","# Instantiate the model\n","model = MultiOutputModel(num_keypoints=6, num_classes=3)\n","\n","# Define loss functions for both outputs\n","criterion_keypoints = nn.MSELoss()  # Mean Squared Error for keypoint regression\n","criterion_class = nn.CrossEntropyLoss()  # Cross-Entropy for classification\n","\n","# Define optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","# Define training loop\n","def train_model(model, train_loader, val_loader, epochs=50):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_class = 0\n","        total_samples = 0\n","\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            keypoints = labels['keypoints_output'].to(device)\n","            vhs_classes = labels['vhs_class_output'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            pred_keypoints, pred_class = model(images)\n","\n","            # Calculate losses\n","            loss_keypoints = criterion_keypoints(pred_keypoints, keypoints)\n","            loss_class = criterion_class(pred_class, vhs_classes)\n","            loss = loss_keypoints + loss_class  # Combine losses\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Track metrics\n","            running_loss += loss.item() * images.size(0)\n","            _, predicted_classes = torch.max(pred_class, 1)\n","            correct_class += (predicted_classes == vhs_classes).sum().item()\n","            total_samples += vhs_classes.size(0)\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_class / total_samples\n","        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n","\n","    print(\"Training Complete\")\n","\n","# Train the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","train_model(model, train_loader, val_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEheXfU1SveV"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Define the path to your test images\n","test_images_dir = '/content/drive/MyDrive/Test_Images/Images'\n","output_csv_path = '/content/drive/MyDrive/vhs_results.csv'\n","\n","# Define the same transformations used during training\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Adjust size based on your model\n","    transforms.ToTensor(),\n","])\n","\n","# Function to calculate VHS from keypoint coordinates\n","def calculate_vhs(keypoints):\n","    A = keypoints[0]\n","    B = keypoints[1]\n","    C = keypoints[2]\n","    D = keypoints[3]\n","    E = keypoints[4]\n","    F = keypoints[5]\n","\n","    # Calculate distances\n","    AB = torch.norm(A - B)  # Distance between points A and B\n","    CD = torch.norm(C - D)  # Distance between points C and D\n","    EF = torch.norm(E - F)  # Distance between points E and F\n","\n","    # Calculate VHS\n","    if EF != 0:  # Prevent division by zero\n","        vhs = 6 * (AB + CD) / EF\n","    else:\n","        vhs = float('inf')  # or handle as you prefer\n","\n","    return vhs.item()  # Return as Python float\n","\n","# Prepare a list to collect results\n","results = []\n","\n","# Switch model to evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    for image_name in os.listdir(test_images_dir):\n","        image_path = os.path.join(test_images_dir, image_name)\n","\n","        # Open and preprocess the image\n","        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n","        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to device\n","\n","        # Get keypoints predictions\n","        pred_keypoints, _ = model(image)  # Only need keypoints\n","        keypoints = pred_keypoints.squeeze(0)  # Remove batch dimension\n","\n","        # Calculate VHS\n","        vhs_value = calculate_vhs(keypoints)\n","\n","        # Append result\n","        results.append({'image_name': image_name, 'vhs': vhs_value})\n","\n","# Convert results to a DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Save to CSV\n","results_df.to_csv(output_csv_path, index=False)\n","\n","print(\"VHS calculation complete. Results saved to:\", output_csv_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00W_xJXVTT7b"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the CSV file\n","csv_file_path = '/content/drive/MyDrive/vhs_results.csv'  # Change this to your file path\n","results = pd.read_csv(csv_file_path)\n","\n","# Display the contents of the DataFrame\n","print(results)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjIBrWmlXx9l"},"outputs":[],"source":["import pandas as pd\n","\n","# Specify the path to your CSV file\n","csv_file_path = '/content/drive/MyDrive/vhs_results.csv'  # Replace with your actual file path\n","new_csv_file_path = '/content/drive/MyDrive/vhs_results.csv'  # Specify the path for the modified CSV file\n","\n","# Load the CSV file\n","df = pd.read_csv(csv_file_path)\n","\n","# Remove the first row (header)\n","df_without_header = df.iloc[1:]\n","\n","# Save the modified DataFrame back to a new CSV file without the index\n","df_without_header.to_csv(new_csv_file_path, index=False)\n","\n","print(\"The first row has been removed and the new CSV file is saved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_jQShcxWUzE"},"outputs":[],"source":["import pandas as pd\n","\n","# Specify the path to your CSV file\n","csv_file_path = '/path/to/your/file.csv'  # Replace with your actual file path\n","\n","# Load the CSV file\n","df = pd.read_csv(csv_file_path)\n","\n","# Get the number of rows\n","num_rows = df.shape[0]\n","\n","# Print the number of rows\n","print(f\"The number of rows in the CSV file is: {num_rows}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMpwbxNmYk_F"},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","import scipy.io\n","import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","IMG_WIDTH, IMG_HEIGHT = 224, 224  # Resize dimensions\n","\n","# Function to classify VHS value into classes\n","def classify_vhs(vhs_value):\n","    if vhs_value < 8.2:\n","        return 0\n","    elif 8.2 <= vhs_value <= 10:\n","        return 1\n","    else:\n","        return 2\n","\n","# Function to load images and labels\n","def load_dataset(image_dir, label_dir):\n","    images = []\n","    keypoints = []\n","    vhs_classes = []\n","\n","    for label_filename in os.listdir(label_dir):\n","        if label_filename.endswith('.mat'):\n","            label_path = os.path.join(label_dir, label_filename)\n","            label_data = scipy.io.loadmat(label_path)\n","            six_points = label_data['six_points']\n","            vhs_value = label_data['VHS'][0, 0]\n","\n","            # Calculate VHS class\n","            vhs_class = classify_vhs(vhs_value)\n","\n","            # Load corresponding image (check for .jpg or .png)\n","            base_filename = os.path.splitext(label_filename)[0]\n","            image_path_jpg = os.path.join(image_dir, base_filename + '.jpg')\n","            image_path_png = os.path.join(image_dir, base_filename + '.png')\n","\n","            if os.path.exists(image_path_jpg):\n","                image = cv2.imread(image_path_jpg)\n","            elif os.path.exists(image_path_png):\n","                image = cv2.imread(image_path_png)\n","            else:\n","                print(f\"Image {base_filename} not found in .jpg or .png format, skipping...\")\n","                continue\n","\n","            # Resize and normalize the image\n","            image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n","\n","            # Scale keypoints to match resized image\n","            scale_x = IMG_WIDTH / image.shape[1]\n","            scale_y = IMG_HEIGHT / image.shape[0]\n","            scaled_kp = six_points * [scale_x, scale_y]\n","\n","            # Append to lists\n","            images.append(image)\n","            keypoints.append(scaled_kp.flatten())\n","            vhs_classes.append(vhs_class)\n","\n","    return np.array(images), np.array(keypoints), np.array(vhs_classes)\n","\n","# Define paths for training and validation data\n","train_image_dir = '/content/drive/MyDrive/Train/Images'\n","train_label_dir = '/content/drive/MyDrive/Train/Labels'\n","valid_image_dir = '/content/drive/MyDrive/Valid/Images'\n","valid_label_dir = '/content/drive/MyDrive/Valid/Labels'\n","\n","# Load the training and validation data\n","train_images, train_keypoints, train_vhs_classes = load_dataset(train_image_dir, train_label_dir)\n","valid_images, valid_keypoints, valid_vhs_classes = load_dataset(valid_image_dir, valid_label_dir)\n","\n","# Check the data\n","print(f\"Training images: {train_images.shape}, Keypoints: {train_keypoints.shape}, VHS classes: {train_vhs_classes.shape}\")\n","print(f\"Validation images: {valid_images.shape}, Keypoints: {valid_keypoints.shape}, VHS classes: {valid_vhs_classes.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-bVUQj4ofRp"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling1D, Lambda, Layer\n","from tensorflow.keras.models import Model\n","from transformers import TFAutoModelForImageClassification, AutoConfig\n","\n","# Define input shape\n","IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 224, 224, 3\n","NUM_CLASSES = 3  # VHS classification (0, 1, 2)\n","NUM_KEYPOINTS = 12  # Six points with x, y coordinates\n","\n","# Load the Swin Transformer with an appropriate configuration, allowing mismatched sizes\n","swin_config = AutoConfig.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", num_labels=NUM_CLASSES)\n","swin_backbone = TFAutoModelForImageClassification.from_pretrained(\n","    \"microsoft/swin-tiny-patch4-window7-224\",\n","    config=swin_config,\n","    ignore_mismatched_sizes=True  # Ignore mismatched size warning\n",")\n","\n","# Custom layer to handle KerasTensor compatibility\n","class SwinTransformerLayer(Layer):\n","    def __init__(self, swin_model):\n","        super(SwinTransformerLayer, self).__init__()\n","        self.swin_model = swin_model\n","\n","    def call(self, inputs, training=False):\n","        # Convert input to tf.Tensor and pass through the Swin model\n","        inputs = tf.convert_to_tensor(inputs)\n","        return self.swin_model(inputs, training=training).logits\n","\n","# Define inputs\n","inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n","\n","# Wrap the normalization function in a Lambda layer\n","normalized_inputs = Lambda(lambda x: (tf.image.convert_image_dtype(x, tf.float32) - 0.5) / 0.5)(inputs)\n","\n","# Pass normalized input through the custom SwinTransformerLayer\n","swin_output = SwinTransformerLayer(swin_backbone)(normalized_inputs)\n","\n","# Global pooling layer for regression head\n","global_avg_pool = GlobalAveragePooling1D()(tf.expand_dims(swin_output, -1))\n","\n","# Regression head for keypoint prediction\n","keypoint_output = Dense(128, activation=\"relu\")(global_avg_pool)\n","keypoint_output = Dropout(0.3)(keypoint_output)\n","keypoint_output = Dense(NUM_KEYPOINTS, activation=\"linear\", name=\"keypoints\")(keypoint_output)\n","\n","# Classification head for VHS class\n","classification_output = Dense(NUM_CLASSES, activation=\"softmax\", name=\"vhs_class\")(swin_output)\n","\n","# Define model with dual outputs\n","model = Model(inputs=inputs, outputs=[keypoint_output, classification_output])\n","\n","# Compile the model with weighted loss\n","losses = {\n","    \"keypoints\": \"mean_squared_error\",    # Regression loss\n","    \"vhs_class\": \"sparse_categorical_crossentropy\"  # Classification loss\n","}\n","loss_weights = {\"keypoints\": 0.5, \"vhs_class\": 0.5}  # Adjust weights to balance tasks\n","\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights, metrics={\"vhs_class\": \"accuracy\"})\n","\n","# Model summary for verification\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5gv5mqWlJ_j"},"outputs":[],"source":["# Define the multi-output model function\n","def create_multi_output_model():\n","    # Load pre-trained MobileNetV2 as base model (without the top layers)\n","    base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n","    base_model.trainable = False  # Freeze the base model layers to retain pretrained weights\n","\n","    # Add global average pooling and a dense layer for feature extraction\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(512, activation='relu')(x)\n","\n","    # Keypoint regression head for predicting coordinates (6 points = 12 values)\n","    kp_output = Dense(12, activation='linear', name='keypoints_output')(x)\n","\n","    # Classification head for predicting the VHS class (3 classes)\n","    class_output = Dense(3, activation='softmax', name='vhs_class_output')(x)\n","\n","    # Define the model with two outputs: keypoints and class prediction\n","    model = Model(inputs=base_model.input, outputs=[kp_output, class_output])\n","\n","    return model\n","\n","# Create the multi-output model\n","model = create_multi_output_model()\n","\n","# Compile the model with appropriate loss functions and metrics for both outputs\n","model.compile(\n","    optimizer=Adam(learning_rate=0.0001),\n","    loss={'keypoints_output': 'mean_squared_error', 'vhs_class_output': 'sparse_categorical_crossentropy'},\n","    metrics={'keypoints_output': 'mae', 'vhs_class_output': 'accuracy'}\n",")\n","\n","# Print model summary\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KwXtdx1XL-2"},"outputs":[],"source":["# Prepare the labels for multi-output training\n","train_labels = {\n","    'keypoints_output': train_keypoints,      # For regression of keypoints\n","    'vhs_class_output': train_vhs_classes     # For classification of VHS class\n","}\n","\n","valid_labels = {\n","    'keypoints_output': valid_keypoints,\n","    'vhs_class_output': valid_vhs_classes\n","}\n","\n","# Train the model with validation data\n","epochs = 50\n","batch_size = 16\n","\n","history = model.fit(\n","    train_images, train_labels,\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    validation_data=(valid_images, valid_labels)\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXrVT4D5XSng"},"outputs":[],"source":["import numpy as np\n","import cv2\n","import os\n","import scipy.io\n","\n","# Define the test image directory\n","test_image_dir = '/content/drive/MyDrive/Test_Images/Images'\n","IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 224, 224, 3\n","# Load the trained model\n","# If the model is saved, you can load it with:\n","# model = tf.keras.models.load_model('path_to_saved_model')\n","\n","# Function to preprocess the test images\n","def preprocess_image(image_path):\n","    image = cv2.imread(image_path)\n","    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n","    return image\n","\n","# Function to calculate the VHS based on predicted key points\n","def calculate_vhs(points):\n","    # Points should be ordered as: A, B, C, D, E, F\n","    AB = np.linalg.norm(points[0] - points[1])\n","    CD = np.linalg.norm(points[2] - points[3])\n","    EF = np.linalg.norm(points[4] - points[5])\n","    vhs = 6 * (AB + CD) / EF\n","    return vhs\n","\n","# Predict and calculate VHS for each image in the test set\n","test_predictions = []\n","for filename in os.listdir(test_image_dir):\n","    if filename.endswith('.jpg') or filename.endswith('.png'):\n","        # Load and preprocess the image\n","        image_path = os.path.join(test_image_dir, filename)\n","        image = preprocess_image(image_path)\n","        image = np.expand_dims(image, axis=0)  # Add batch dimension\n","\n","        # Predict the key points and class\n","        keypoint_preds, class_preds = model.predict(image)\n","\n","        # Reshape keypoint predictions into (6, 2) and rescale to original image size\n","        keypoints = keypoint_preds[0].reshape((6, 2))\n","        scale_x = image.shape[2] / IMG_WIDTH\n","        scale_y = image.shape[1] / IMG_HEIGHT\n","        scaled_keypoints = keypoints * [scale_x, scale_y]\n","\n","        # Calculate VHS\n","        vhs_value = calculate_vhs(scaled_keypoints)\n","\n","        # Save the predictions\n","        test_predictions.append({\n","            'filename': filename,\n","            'predicted_keypoints': scaled_keypoints,\n","            'predicted_vhs': vhs_value,\n","            'predicted_class': np.argmax(class_preds[0])  # The VHS class prediction\n","        })\n","\n","# Display predictions\n","for prediction in test_predictions:\n","    print(f\"Image: {prediction['filename']}\")\n","    print(f\"Predicted Keypoints:\\n{prediction['predicted_keypoints']}\")\n","    print(f\"Predicted VHS: {prediction['predicted_vhs']:.2f}\")\n","    print(f\"Predicted VHS Class: {prediction['predicted_class']}\")\n","    print(\"-\" * 30)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}